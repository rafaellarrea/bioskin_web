# C√≥digo del Servidor PaliGemma para Google Colab
# Copia y pega este c√≥digo en una NUEVA celda en tu notebook de Colab, DEBAJO del script de configuraci√≥n.

import torch
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from pydantic import BaseModel
from PIL import Image
import io
import uvicorn
from pyngrok import ngrok
import nest_asyncio
import os

# 1. Configuraci√≥n del Modelo
# OPCI√ìN A: PaliGemma (Generalista - 3B)
def load_paligemma_model():
    print("‚è≥ Cargando modelo PaliGemma (google/paligemma-3b-mix-224)...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    model_id = "google/paligemma-3b-mix-224"
    processor = AutoProcessor.from_pretrained(model_id)
    model = PaliGemmaForConditionalGeneration.from_pretrained(
        model_id,
        quantization_config=quantization_config,
        device_map="auto"
    )
    print("‚úÖ Modelo PaliGemma cargado.")
    return model, processor

# OPCI√ìN B: MedGemma (M√©dico - 4B)
# Nota: Aseg√∫rate de que este modelo exista y tengas acceso en Hugging Face
def load_medgemma_model():
    print("‚è≥ Cargando modelo MedGemma (google/medgemma-4b-it)...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16 # Usar float16 para T4, bfloat16 para Ampere
    )
    model_id = "google/medgemma-4b-it" 
    processor = AutoProcessor.from_pretrained(model_id)
    model = PaliGemmaForConditionalGeneration.from_pretrained(
        model_id,
        quantization_config=quantization_config,
        device_map="auto"
    )
    print("‚úÖ Modelo MedGemma cargado.")
    return model, processor

# --- SELECCI√ìN DE MODELO ---
# Descomenta la l√≠nea del modelo que quieras usar:
model, processor = load_paligemma_model()
# model, processor = load_medgemma_model()
# ---------------------------

# 2. Definir la API con FastAPI
app = FastAPI()

@app.get("/")
def read_root():
    return {"status": "online", "model": "PaliGemma-3b-mix-224"}

@app.post("/analyze")
async def analyze_image(prompt: str = Form(...), file: UploadFile = File(...)):
    try:
        # Leer imagen
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert("RGB")
        
        # Preprocesar
        inputs = processor(text=prompt, images=image, return_tensors="pt").to(model.device)
        input_len = inputs["input_ids"].shape[-1]
        
        # Generar
        with torch.inference_mode():
            generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)
            generation = generation[0][input_len:]
            decoded = processor.decode(generation, skip_special_tokens=True)
            
        return {"result": decoded}
        
    except Exception as e:
        print(f"Error en an√°lisis: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# 3. Iniciar Servidor y T√∫nel Ngrok
def start_server():
    # Configurar t√∫nel Ngrok
    port = 8000
    # Matar t√∫neles anteriores si existen
    ngrok.kill()
    
    # Leer token de variable de entorno (configurada en el paso anterior)
    auth_token = os.environ.get("NGROK_AUTHTOKEN")
    if auth_token:
        ngrok.set_auth_token(auth_token)
    
    public_url = ngrok.connect(port).public_url
    print(f"\nüöÄ API P√∫blica Ngrok ACTIVA: {public_url}")
    print(f"üìã COPIA esta URL para usarla en tu aplicaci√≥n.\n")
    
    # Iniciar Uvicorn
    nest_asyncio.apply()
    uvicorn.run(app, host="0.0.0.0", port=port)

if __name__ == "__main__":
    start_server()
